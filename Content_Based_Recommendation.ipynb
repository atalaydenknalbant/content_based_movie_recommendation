{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1513a3",
   "metadata": {},
   "source": [
    "# Content-based Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ce2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search on Matrix Factorization\n",
    "## Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "## Load MovieLens Small dataset\n",
    "data = pd.read_csv(r\"C:\\Users\\yineh\\OneDrive\\Masaüstü\\ml-latest-small\\ratings.csv\", sep=\",\")\n",
    "\n",
    "data.head()\n",
    "## Check shape of data\n",
    "data.shape\n",
    "## Map user and movie IDs to unique consecutive indices starting from 0\n",
    "user_ids = data['userId'].unique()\n",
    "movie_ids = data['movieId'].unique()\n",
    "\n",
    "user_mapping = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "movie_mapping = {movie_id: idx for idx, movie_id in enumerate(movie_ids)}\n",
    "\n",
    "data['userId'] = data['userId'].map(user_mapping)\n",
    "data['movieId'] = data['movieId'].map(movie_mapping)\n",
    "## Create the matrix factorization model\n",
    "class MatrixFactorization(nn.Module):\n",
    "    def __init__(self, num_users, num_movies, embedding_size=20):\n",
    "        super(MatrixFactorization, self).__init__()\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_size)\n",
    "        self.movie_embedding = nn.Embedding(num_movies, embedding_size)\n",
    "        self.linear = nn.Linear(2*embedding_size, 1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        user_embedding = self.user_embedding(X[:,0])\n",
    "        movie_embedding = self.movie_embedding(X[:,1])\n",
    "        prediction = torch.sum(user_embedding * movie_embedding, dim=1)\n",
    "        return prediction\n",
    "## Custom Dataset to use Dataloaders\n",
    "class MovieLensDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.users = torch.tensor(dataframe['userId'].values, dtype=torch.long)\n",
    "        self.movies = torch.tensor(dataframe['movieId'].values, dtype=torch.long)\n",
    "        self.ratings = torch.tensor(dataframe['rating'].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.users[idx], self.movies[idx]), self.ratings[idx]\n",
    "## Define parameter for grid search\n",
    "embedding_sizes = [5, 10, 20, 50]\n",
    "learning_rates = [0.01, 0.03, 0.1, 0.001]\n",
    "regularizations = [0, 0.001, 0.01, 0.1]\n",
    "## Generate all combinations\n",
    "param_grid = list(itertools.product(embedding_sizes, learning_rates, regularizations))\n",
    "## Store Results\n",
    "results = []\n",
    "## Use gpu for calculations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "## Split the data into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "## Prepare datasets and data loaders\n",
    "batch_size = 64\n",
    "train_dataset = MovieLensDataset(train_data)\n",
    "test_dataset = MovieLensDataset(test_data)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "## Loop over all combinations\n",
    "for embedding_size, lr, reg in param_grid:\n",
    "    num_users = len(user_ids)\n",
    "    num_movies = len(movie_ids)\n",
    "    model = MatrixFactorization(num_users, num_movies, embedding_size).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=reg)\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for (users, movies), ratings in train_loader:\n",
    "            users = users.to(device)\n",
    "            movies = movies.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(torch.stack((users, movies), dim=1))\n",
    "            loss = criterion(outputs, ratings)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_preds = []\n",
    "        all_ratings = []\n",
    "        for (users, movies), ratings in test_loader:\n",
    "            users = users.to(device)\n",
    "            movies = movies.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "            \n",
    "            outputs = model(torch.stack((users, movies), dim=1))\n",
    "            all_preds.append(outputs.cpu())\n",
    "            all_ratings.append(ratings.cpu())\n",
    "        \n",
    "        predictions = torch.cat(all_preds)\n",
    "        y_test = torch.cat(all_ratings)\n",
    "        test_mae = nn.L1Loss()(predictions, y_test).item()\n",
    "        test_rmse = torch.sqrt(nn.MSELoss()(predictions, y_test)).item()\n",
    "    results.append({\n",
    "        'Embedding Size': embedding_size,\n",
    "        'Learning Rate': lr,\n",
    "        'Regularization': reg,\n",
    "        'Test MAE': test_mae,\n",
    "        'Test RMSE': test_rmse\n",
    "    })\n",
    "    \n",
    "    print(f\"Params: Embedding Size={embedding_size}, LR={lr}, Reg={reg}\")\n",
    "    print(f\"Test MAE: {test_mae:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "## Summarize Results\n",
    "results_df = pd.DataFrame(results)\n",
    "sorted_results = results_df.sort_values(by='Test MAE')\n",
    "print(\"Top 10 configurations based on Test MAE:\")\n",
    "print(sorted_results.head(10))\n",
    "\n",
    "## Visualizing the Results\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Embedding Size', y='Test MAE', data=results_df)\n",
    "plt.title('Effect of Embedding Size on Test MAE')\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Learning Rate', y='Test MAE', data=results_df)\n",
    "plt.title('Effect of Learning Rate on Test MAE')\n",
    "plt.show()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='Regularization', y='Test MAE', data=results_df)\n",
    "plt.title('Effect of Regularization on Test MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec61265",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ca31c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import torch\n",
    "import os\n",
    "device = torch.device('cuda')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c60c6a",
   "metadata": {},
   "source": [
    "## Initialize OMBD api to fetch plot summaries then save it as plots.csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6d083",
   "metadata": {},
   "source": [
    "## Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0df22cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratings = pd.read_csv(r\"C:\\Users\\yineh\\OneDrive\\Masaüstü\\ml-latest-small\\ratings.csv\")\n",
    "# links = pd.read_csv(r\"C:\\Users\\yineh\\OneDrive\\Masaüstü\\ml-latest-small\\links.csv\")\n",
    "# tags = pd.read_csv(r\"C:\\Users\\yineh\\OneDrive\\Masaüstü\\ml-latest-small\\tags.csv\")\n",
    "# movies = pd.read_csv(r\"C:\\Users\\yineh\\OneDrive\\Masaüstü\\ml-latest-small\\movies.csv\")\n",
    "# plots = pd.read_csv(r\"C:\\Users\\yineh\\OneDrive\\Masaüstü\\ml-latest-small\\plots.csv\")\n",
    "ratings = pd.read_csv(r\"C:\\Users\\yineh\\OneDrive\\Masaüstü\\ml-latest\\ratings.csv\")\n",
    "links = pd.read_csv(r\"C:\\Users\\yineh\\OneDrive\\Masaüstü\\ml-latest\\links.csv\")\n",
    "tags = pd.read_csv(r\"C:\\Users\\yineh\\OneDrive\\Masaüstü\\ml-latest\\tags.csv\")\n",
    "movies = pd.read_csv(r\"C:\\Users\\yineh\\OneDrive\\Masaüstü\\ml-latest\\movies.csv\")\n",
    "plots = pd.read_csv(r\"C:\\Users\\yineh\\OneDrive\\Masaüstü\\ml-latest\\plots.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d62bf7",
   "metadata": {},
   "source": [
    "## Merge movie tags into a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2e141c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_tags = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x.dropna().astype(str))).reset_index()\n",
    "movies = movies.merge(movie_tags, on='movieId', how='inner')\n",
    "movies = movies.merge(plots[['movieId', 'plot_summary']], on='movieId', how='inner')\n",
    "movies = movies.dropna(subset=['tag', 'plot_summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d3c8e3",
   "metadata": {},
   "source": [
    "## Combine Tags and Plot Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa923af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_features(row):\n",
    "    return f\"Movie Plot: {row['plot_summary']} Keywords: {row['tag']}\"\n",
    "\n",
    "movies['plots_tags'] = movies.apply(combine_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe15873",
   "metadata": {},
   "source": [
    "## Clean text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63f0c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text_data(text):\n",
    "#     text = text.lower()\n",
    "#     text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "#     return text\n",
    "\n",
    "# tqdm.pandas(desc=\"Cleaning Text\")\n",
    "# movies['plots_tags'] = movies['plots_tags'].progress_apply(clean_text_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572ce6a2",
   "metadata": {},
   "source": [
    "## Reset Movie Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d856e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.reset_index(drop=True, inplace=True)\n",
    "movie_indices = pd.Series(movies.index, index=movies['movieId']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f409f",
   "metadata": {},
   "source": [
    "## Initialize the pre-trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bc7b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"dunzhang/stella_en_1.5B_v5\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371d3d0",
   "metadata": {},
   "source": [
    "## Generate embeddings for all movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f90ca3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding file not found. Generating embeddings from scratch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings in Batches: 100%|██████████| 829/829 [35:27<00:00,  2.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to movie_embeddings.pt\n"
     ]
    }
   ],
   "source": [
    "embedding_path = 'movie_embeddings.pt'\n",
    "\n",
    "# Function to save embeddings\n",
    "def save_embeddings(embeddings, path=embedding_path):\n",
    "    torch.save(embeddings, path)\n",
    "    print(f\"Embeddings saved to {path}\")\n",
    "\n",
    "# Function to load embeddings\n",
    "def load_embeddings(path=embedding_path):\n",
    "    if os.path.exists(path):\n",
    "        embeddings = torch.load(path, map_location=device)\n",
    "        print(f\"Embeddings loaded from {path}\")\n",
    "        return embeddings\n",
    "    else:\n",
    "        print(\"Embedding file not found. Generating embeddings from scratch.\")\n",
    "        return None\n",
    "\n",
    "# Check if embeddings are already saved; load if available, otherwise generate and save\n",
    "movie_embeddings = load_embeddings()\n",
    "movie_embeddings = movie_embeddings.type(torch.FloatTensor).to(device)\n",
    "if movie_embeddings is None:\n",
    "    # Generate embeddings as before\n",
    "    documents = movies['plots_tags'].tolist()\n",
    "    batch_size = 64\n",
    "    movie_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(documents), batch_size), desc=\"Generating Embeddings in Batches\"):\n",
    "        batch = documents[i:i + batch_size]\n",
    "        batch_embeddings = model.encode(batch, convert_to_tensor=True, show_progress_bar=False)\n",
    "        movie_embeddings.append(batch_embeddings.to(device))\n",
    "\n",
    "    movie_embeddings = torch.cat(movie_embeddings)\n",
    "    \n",
    "    # Save embeddings after generating them\n",
    "    save_embeddings(movie_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2425fe00",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa1d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.0000001 , 0.56306165, 0.5670217 , ..., 0.49139744, 0.5040304 ,\n",
       "       0.4888566 ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cosine_sim = cosine_similarity(movie_embeddings.cpu())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f31f10",
   "metadata": {},
   "source": [
    "## Filter ratings to include only movies present in the movies DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c513b623",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = ratings[ratings['movieId'].isin(movies['movieId'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4cb895",
   "metadata": {},
   "source": [
    "## Predict rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, movie_id):\n",
    "    user_ratings = ratings[ratings['userId'] == user_id]\n",
    "    user_movie_ids = user_ratings['movieId'].values\n",
    "\n",
    "    if movie_id not in movie_indices.index or len(user_movie_ids) == 0:\n",
    "        return ratings['rating'].mean()\n",
    "\n",
    "    idx = movie_indices[movie_id]\n",
    "    target_vector = movie_embeddings[idx].unsqueeze(0)\n",
    "\n",
    "    user_indices = user_ratings['movieId'].map(movie_indices).dropna().astype(int)\n",
    "    if len(user_indices) == 0:\n",
    "        return ratings['rating'].mean()\n",
    "\n",
    "    user_indices_list = user_indices.tolist()\n",
    "    user_vectors = movie_embeddings[user_indices_list]\n",
    "    ratings_values = user_ratings['rating'].values\n",
    "\n",
    "    # Convert ratings to tensor\n",
    "    ratings_tensor = torch.tensor(ratings_values, device=device)\n",
    "\n",
    "    # Compute similarities on GPU\n",
    "    similarities = torch.nn.functional.cosine_similarity(target_vector, user_vectors)\n",
    "\n",
    "    # Handle zero similarity case\n",
    "    if torch.sum(similarities) == 0:\n",
    "        return ratings['rating'].mean()\n",
    "\n",
    "    # Compute predicted rating\n",
    "    predicted_rating = torch.dot(similarities, ratings_tensor) / torch.sum(similarities)\n",
    "    return predicted_rating.item()\n",
    "\n",
    "\n",
    "ratings_list = ratings.to_dict('records')\n",
    "\n",
    "def predict_rating_wrapper(row):\n",
    "    return predict_rating(row['userId'], row['movieId'])\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(tqdm(executor.map(predict_rating_wrapper, ratings_list), total=len(ratings_list), desc=\"Predicting Ratings\"))\n",
    "\n",
    "ratings['predicted_rating'] = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe516dc7",
   "metadata": {},
   "source": [
    "## Calculate Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88d2a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.663473670155574\n"
     ]
    }
   ],
   "source": [
    "mae = mean_absolute_error(ratings['rating'], ratings['predicted_rating'])\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142b809",
   "metadata": {},
   "source": [
    "## Top-N Recommendation and Hit Ratio Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68ac42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) \n",
    "\n",
    "positive_preferences = ratings\n",
    "test_size = 1000\n",
    "test_indices = np.random.choice(positive_preferences.index, size=test_size, replace=False)\n",
    "test_set = positive_preferences.loc[test_indices]\n",
    "\n",
    "train_set = ratings.drop(test_indices)\n",
    "ratings = train_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad9346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recommendations(user_id, N=10):\n",
    "    user_rated_movie_ids = ratings[ratings['userId'] == user_id]['movieId'].tolist()\n",
    "    user_indices = ratings[ratings['userId'] == user_id]['movieId'].map(movie_indices).dropna().astype(int)\n",
    "    user_ratings_values = ratings[ratings['userId'] == user_id]['rating'].values\n",
    "\n",
    "    if len(user_indices) == 0:\n",
    "        return []\n",
    "\n",
    "    user_indices_list = user_indices.tolist()\n",
    "    user_feature_vectors = movie_embeddings[user_indices_list]\n",
    "\n",
    "    # Convert ratings to tensor\n",
    "    user_ratings_tensor = torch.tensor(user_ratings_values, device=device)\n",
    "\n",
    "    # Compute user profile on GPU\n",
    "    user_profile = torch.sum(user_feature_vectors * user_ratings_tensor.unsqueeze(1), dim=0) / torch.sum(user_ratings_tensor)\n",
    "\n",
    "    # Compute similarities on GPU\n",
    "    similarities = torch.nn.functional.cosine_similarity(user_profile.unsqueeze(0), movie_embeddings).squeeze(0)\n",
    "\n",
    "    # Move similarities to CPU for further processing\n",
    "    similarities = similarities.cpu().numpy()\n",
    "\n",
    "    # Filter out movies already rated by the user\n",
    "    candidate_indices = [idx for idx in range(len(movies)) if movies.loc[idx, 'movieId'] not in user_rated_movie_ids]\n",
    "    candidate_similarities = similarities[candidate_indices]\n",
    "\n",
    "    # Get top N recommendations\n",
    "    top_N_indices = np.argsort(candidate_similarities)[-N:][::-1]\n",
    "    top_N_movie_indices = [candidate_indices[i] for i in top_N_indices]\n",
    "    top_N_movie_ids = movies.loc[top_N_movie_indices, 'movieId'].tolist()\n",
    "\n",
    "    return top_N_movie_ids\n",
    "\n",
    "\n",
    "def evaluate_top_n_recommendations(test_set, N=10):\n",
    "    total = len(test_set)\n",
    "\n",
    "    def evaluate_row(row):\n",
    "        user_id = row['userId']\n",
    "        test_movie_id = row['movieId']\n",
    "        recommended_movie_ids = get_top_n_recommendations(user_id, N)\n",
    "        return 1 if test_movie_id in recommended_movie_ids else 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        results = list(tqdm(executor.map(evaluate_row, (row for _, row in test_set.iterrows())), total=total, desc=\"Evaluating Recommendations\"))\n",
    "\n",
    "    hits = sum(results)\n",
    "    hit_ratio = hits / total\n",
    "    return hit_ratio\n",
    "\n",
    "N = 10\n",
    "hit_ratio = evaluate_top_n_recommendations(test_set, N)\n",
    "print(f'Hit Ratio: {hit_ratio}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
